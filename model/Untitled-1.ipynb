{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pprint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13568/3901302674.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtransformer_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_encoder_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pprint' is not defined"
     ]
    }
   ],
   "source": [
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "pprint(transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adv_inception_v3',\n",
      " 'cait_m36_384',\n",
      " 'cait_m48_448',\n",
      " 'cait_s24_224',\n",
      " 'cait_s24_384',\n",
      " 'cait_s36_384',\n",
      " 'cait_xs24_384',\n",
      " 'cait_xxs24_224',\n",
      " 'cait_xxs24_384',\n",
      " 'cait_xxs36_224',\n",
      " 'cait_xxs36_384',\n",
      " 'coat_lite_mini',\n",
      " 'coat_lite_small',\n",
      " 'coat_lite_tiny',\n",
      " 'coat_mini',\n",
      " 'coat_tiny',\n",
      " 'convit_base',\n",
      " 'convit_small',\n",
      " 'convit_tiny',\n",
      " 'cspdarknet53',\n",
      " 'cspresnet50',\n",
      " 'cspresnext50',\n",
      " 'deit_base_distilled_patch16_224',\n",
      " 'deit_base_distilled_patch16_384',\n",
      " 'deit_base_patch16_224',\n",
      " 'deit_base_patch16_384',\n",
      " 'deit_small_distilled_patch16_224',\n",
      " 'deit_small_patch16_224',\n",
      " 'deit_tiny_distilled_patch16_224',\n",
      " 'deit_tiny_patch16_224',\n",
      " 'densenet121',\n",
      " 'densenet161',\n",
      " 'densenet169',\n",
      " 'densenet201',\n",
      " 'densenetblur121d',\n",
      " 'dla34',\n",
      " 'dla46_c',\n",
      " 'dla46x_c',\n",
      " 'dla60',\n",
      " 'dla60_res2net',\n",
      " 'dla60_res2next',\n",
      " 'dla60x',\n",
      " 'dla60x_c',\n",
      " 'dla102',\n",
      " 'dla102x',\n",
      " 'dla102x2',\n",
      " 'dla169',\n",
      " 'dm_nfnet_f0',\n",
      " 'dm_nfnet_f1',\n",
      " 'dm_nfnet_f2',\n",
      " 'dm_nfnet_f3',\n",
      " 'dm_nfnet_f4',\n",
      " 'dm_nfnet_f5',\n",
      " 'dm_nfnet_f6',\n",
      " 'dpn68',\n",
      " 'dpn68b',\n",
      " 'dpn92',\n",
      " 'dpn98',\n",
      " 'dpn107',\n",
      " 'dpn131',\n",
      " 'eca_nfnet_l0',\n",
      " 'eca_nfnet_l1',\n",
      " 'eca_nfnet_l2',\n",
      " 'ecaresnet26t',\n",
      " 'ecaresnet50d',\n",
      " 'ecaresnet50d_pruned',\n",
      " 'ecaresnet50t',\n",
      " 'ecaresnet101d',\n",
      " 'ecaresnet101d_pruned',\n",
      " 'ecaresnet269d',\n",
      " 'ecaresnetlight',\n",
      " 'efficientnet_b0',\n",
      " 'efficientnet_b1',\n",
      " 'efficientnet_b1_pruned',\n",
      " 'efficientnet_b2',\n",
      " 'efficientnet_b2_pruned',\n",
      " 'efficientnet_b3',\n",
      " 'efficientnet_b3_pruned',\n",
      " 'efficientnet_b4',\n",
      " 'efficientnet_el',\n",
      " 'efficientnet_el_pruned',\n",
      " 'efficientnet_em',\n",
      " 'efficientnet_es',\n",
      " 'efficientnet_es_pruned',\n",
      " 'efficientnet_lite0',\n",
      " 'efficientnetv2_rw_m',\n",
      " 'efficientnetv2_rw_s',\n",
      " 'ens_adv_inception_resnet_v2',\n",
      " 'ese_vovnet19b_dw',\n",
      " 'ese_vovnet39b',\n",
      " 'fbnetc_100',\n",
      " 'gernet_l',\n",
      " 'gernet_m',\n",
      " 'gernet_s',\n",
      " 'ghostnet_100',\n",
      " 'gluon_inception_v3',\n",
      " 'gluon_resnet18_v1b',\n",
      " 'gluon_resnet34_v1b',\n",
      " 'gluon_resnet50_v1b',\n",
      " 'gluon_resnet50_v1c',\n",
      " 'gluon_resnet50_v1d',\n",
      " 'gluon_resnet50_v1s',\n",
      " 'gluon_resnet101_v1b',\n",
      " 'gluon_resnet101_v1c',\n",
      " 'gluon_resnet101_v1d',\n",
      " 'gluon_resnet101_v1s',\n",
      " 'gluon_resnet152_v1b',\n",
      " 'gluon_resnet152_v1c',\n",
      " 'gluon_resnet152_v1d',\n",
      " 'gluon_resnet152_v1s',\n",
      " 'gluon_resnext50_32x4d',\n",
      " 'gluon_resnext101_32x4d',\n",
      " 'gluon_resnext101_64x4d',\n",
      " 'gluon_senet154',\n",
      " 'gluon_seresnext50_32x4d',\n",
      " 'gluon_seresnext101_32x4d',\n",
      " 'gluon_seresnext101_64x4d',\n",
      " 'gluon_xception65',\n",
      " 'gmixer_24_224',\n",
      " 'gmlp_s16_224',\n",
      " 'hardcorenas_a',\n",
      " 'hardcorenas_b',\n",
      " 'hardcorenas_c',\n",
      " 'hardcorenas_d',\n",
      " 'hardcorenas_e',\n",
      " 'hardcorenas_f',\n",
      " 'hrnet_w18',\n",
      " 'hrnet_w18_small',\n",
      " 'hrnet_w18_small_v2',\n",
      " 'hrnet_w30',\n",
      " 'hrnet_w32',\n",
      " 'hrnet_w40',\n",
      " 'hrnet_w44',\n",
      " 'hrnet_w48',\n",
      " 'hrnet_w64',\n",
      " 'ig_resnext101_32x8d',\n",
      " 'ig_resnext101_32x16d',\n",
      " 'ig_resnext101_32x32d',\n",
      " 'ig_resnext101_32x48d',\n",
      " 'inception_resnet_v2',\n",
      " 'inception_v3',\n",
      " 'inception_v4',\n",
      " 'legacy_senet154',\n",
      " 'legacy_seresnet18',\n",
      " 'legacy_seresnet34',\n",
      " 'legacy_seresnet50',\n",
      " 'legacy_seresnet101',\n",
      " 'legacy_seresnet152',\n",
      " 'legacy_seresnext26_32x4d',\n",
      " 'legacy_seresnext50_32x4d',\n",
      " 'legacy_seresnext101_32x4d',\n",
      " 'levit_128',\n",
      " 'levit_128s',\n",
      " 'levit_192',\n",
      " 'levit_256',\n",
      " 'levit_384',\n",
      " 'mixer_b16_224',\n",
      " 'mixer_b16_224_in21k',\n",
      " 'mixer_b16_224_miil',\n",
      " 'mixer_b16_224_miil_in21k',\n",
      " 'mixer_l16_224',\n",
      " 'mixer_l16_224_in21k',\n",
      " 'mixnet_l',\n",
      " 'mixnet_m',\n",
      " 'mixnet_s',\n",
      " 'mixnet_xl',\n",
      " 'mnasnet_100',\n",
      " 'mobilenetv2_100',\n",
      " 'mobilenetv2_110d',\n",
      " 'mobilenetv2_120d',\n",
      " 'mobilenetv2_140',\n",
      " 'mobilenetv3_large_100',\n",
      " 'mobilenetv3_large_100_miil',\n",
      " 'mobilenetv3_large_100_miil_in21k',\n",
      " 'mobilenetv3_rw',\n",
      " 'nasnetalarge',\n",
      " 'nf_regnet_b1',\n",
      " 'nf_resnet50',\n",
      " 'nfnet_l0',\n",
      " 'pit_b_224',\n",
      " 'pit_b_distilled_224',\n",
      " 'pit_s_224',\n",
      " 'pit_s_distilled_224',\n",
      " 'pit_ti_224',\n",
      " 'pit_ti_distilled_224',\n",
      " 'pit_xs_224',\n",
      " 'pit_xs_distilled_224',\n",
      " 'pnasnet5large',\n",
      " 'regnetx_002',\n",
      " 'regnetx_004',\n",
      " 'regnetx_006',\n",
      " 'regnetx_008',\n",
      " 'regnetx_016',\n",
      " 'regnetx_032',\n",
      " 'regnetx_040',\n",
      " 'regnetx_064',\n",
      " 'regnetx_080',\n",
      " 'regnetx_120',\n",
      " 'regnetx_160',\n",
      " 'regnetx_320',\n",
      " 'regnety_002',\n",
      " 'regnety_004',\n",
      " 'regnety_006',\n",
      " 'regnety_008',\n",
      " 'regnety_016',\n",
      " 'regnety_032',\n",
      " 'regnety_040',\n",
      " 'regnety_064',\n",
      " 'regnety_080',\n",
      " 'regnety_120',\n",
      " 'regnety_160',\n",
      " 'regnety_320',\n",
      " 'repvgg_a2',\n",
      " 'repvgg_b0',\n",
      " 'repvgg_b1',\n",
      " 'repvgg_b1g4',\n",
      " 'repvgg_b2',\n",
      " 'repvgg_b2g4',\n",
      " 'repvgg_b3',\n",
      " 'repvgg_b3g4',\n",
      " 'res2net50_14w_8s',\n",
      " 'res2net50_26w_4s',\n",
      " 'res2net50_26w_6s',\n",
      " 'res2net50_26w_8s',\n",
      " 'res2net50_48w_2s',\n",
      " 'res2net101_26w_4s',\n",
      " 'res2next50',\n",
      " 'resmlp_12_224',\n",
      " 'resmlp_12_distilled_224',\n",
      " 'resmlp_24_224',\n",
      " 'resmlp_24_distilled_224',\n",
      " 'resmlp_36_224',\n",
      " 'resmlp_36_distilled_224',\n",
      " 'resmlp_big_24_224',\n",
      " 'resmlp_big_24_224_in22ft1k',\n",
      " 'resmlp_big_24_distilled_224',\n",
      " 'resnest14d',\n",
      " 'resnest26d',\n",
      " 'resnest50d',\n",
      " 'resnest50d_1s4x24d',\n",
      " 'resnest50d_4s2x40d',\n",
      " 'resnest101e',\n",
      " 'resnest200e',\n",
      " 'resnest269e',\n",
      " 'resnet18',\n",
      " 'resnet18d',\n",
      " 'resnet26',\n",
      " 'resnet26d',\n",
      " 'resnet34',\n",
      " 'resnet34d',\n",
      " 'resnet50',\n",
      " 'resnet50d',\n",
      " 'resnet51q',\n",
      " 'resnet101d',\n",
      " 'resnet152d',\n",
      " 'resnet200d',\n",
      " 'resnetblur50',\n",
      " 'resnetrs50',\n",
      " 'resnetrs101',\n",
      " 'resnetrs152',\n",
      " 'resnetrs200',\n",
      " 'resnetrs270',\n",
      " 'resnetrs350',\n",
      " 'resnetrs420',\n",
      " 'resnetv2_50x1_bit_distilled',\n",
      " 'resnetv2_50x1_bitm',\n",
      " 'resnetv2_50x1_bitm_in21k',\n",
      " 'resnetv2_50x3_bitm',\n",
      " 'resnetv2_50x3_bitm_in21k',\n",
      " 'resnetv2_101x1_bitm',\n",
      " 'resnetv2_101x1_bitm_in21k',\n",
      " 'resnetv2_101x3_bitm',\n",
      " 'resnetv2_101x3_bitm_in21k',\n",
      " 'resnetv2_152x2_bit_teacher',\n",
      " 'resnetv2_152x2_bit_teacher_384',\n",
      " 'resnetv2_152x2_bitm',\n",
      " 'resnetv2_152x2_bitm_in21k',\n",
      " 'resnetv2_152x4_bitm',\n",
      " 'resnetv2_152x4_bitm_in21k',\n",
      " 'resnext50_32x4d',\n",
      " 'resnext50d_32x4d',\n",
      " 'resnext101_32x8d',\n",
      " 'rexnet_100',\n",
      " 'rexnet_130',\n",
      " 'rexnet_150',\n",
      " 'rexnet_200',\n",
      " 'selecsls42b',\n",
      " 'selecsls60',\n",
      " 'selecsls60b',\n",
      " 'semnasnet_100',\n",
      " 'seresnet50',\n",
      " 'seresnet152d',\n",
      " 'seresnext26d_32x4d',\n",
      " 'seresnext26t_32x4d',\n",
      " 'seresnext50_32x4d',\n",
      " 'skresnet18',\n",
      " 'skresnet34',\n",
      " 'skresnext50_32x4d',\n",
      " 'spnasnet_100',\n",
      " 'ssl_resnet18',\n",
      " 'ssl_resnet50',\n",
      " 'ssl_resnext50_32x4d',\n",
      " 'ssl_resnext101_32x4d',\n",
      " 'ssl_resnext101_32x8d',\n",
      " 'ssl_resnext101_32x16d',\n",
      " 'swin_base_patch4_window7_224',\n",
      " 'swin_base_patch4_window7_224_in22k',\n",
      " 'swin_base_patch4_window12_384',\n",
      " 'swin_base_patch4_window12_384_in22k',\n",
      " 'swin_large_patch4_window7_224',\n",
      " 'swin_large_patch4_window7_224_in22k',\n",
      " 'swin_large_patch4_window12_384',\n",
      " 'swin_large_patch4_window12_384_in22k',\n",
      " 'swin_small_patch4_window7_224',\n",
      " 'swin_tiny_patch4_window7_224',\n",
      " 'swsl_resnet18',\n",
      " 'swsl_resnet50',\n",
      " 'swsl_resnext50_32x4d',\n",
      " 'swsl_resnext101_32x4d',\n",
      " 'swsl_resnext101_32x8d',\n",
      " 'swsl_resnext101_32x16d',\n",
      " 'tf_efficientnet_b0',\n",
      " 'tf_efficientnet_b0_ap',\n",
      " 'tf_efficientnet_b0_ns',\n",
      " 'tf_efficientnet_b1',\n",
      " 'tf_efficientnet_b1_ap',\n",
      " 'tf_efficientnet_b1_ns',\n",
      " 'tf_efficientnet_b2',\n",
      " 'tf_efficientnet_b2_ap',\n",
      " 'tf_efficientnet_b2_ns',\n",
      " 'tf_efficientnet_b3',\n",
      " 'tf_efficientnet_b3_ap',\n",
      " 'tf_efficientnet_b3_ns',\n",
      " 'tf_efficientnet_b4',\n",
      " 'tf_efficientnet_b4_ap',\n",
      " 'tf_efficientnet_b4_ns',\n",
      " 'tf_efficientnet_b5',\n",
      " 'tf_efficientnet_b5_ap',\n",
      " 'tf_efficientnet_b5_ns',\n",
      " 'tf_efficientnet_b6',\n",
      " 'tf_efficientnet_b6_ap',\n",
      " 'tf_efficientnet_b6_ns',\n",
      " 'tf_efficientnet_b7',\n",
      " 'tf_efficientnet_b7_ap',\n",
      " 'tf_efficientnet_b7_ns',\n",
      " 'tf_efficientnet_b8',\n",
      " 'tf_efficientnet_b8_ap',\n",
      " 'tf_efficientnet_cc_b0_4e',\n",
      " 'tf_efficientnet_cc_b0_8e',\n",
      " 'tf_efficientnet_cc_b1_8e',\n",
      " 'tf_efficientnet_el',\n",
      " 'tf_efficientnet_em',\n",
      " 'tf_efficientnet_es',\n",
      " 'tf_efficientnet_l2_ns',\n",
      " 'tf_efficientnet_l2_ns_475',\n",
      " 'tf_efficientnet_lite0',\n",
      " 'tf_efficientnet_lite1',\n",
      " 'tf_efficientnet_lite2',\n",
      " 'tf_efficientnet_lite3',\n",
      " 'tf_efficientnet_lite4',\n",
      " 'tf_efficientnetv2_b0',\n",
      " 'tf_efficientnetv2_b1',\n",
      " 'tf_efficientnetv2_b2',\n",
      " 'tf_efficientnetv2_b3',\n",
      " 'tf_efficientnetv2_l',\n",
      " 'tf_efficientnetv2_l_in21ft1k',\n",
      " 'tf_efficientnetv2_l_in21k',\n",
      " 'tf_efficientnetv2_m',\n",
      " 'tf_efficientnetv2_m_in21ft1k',\n",
      " 'tf_efficientnetv2_m_in21k',\n",
      " 'tf_efficientnetv2_s',\n",
      " 'tf_efficientnetv2_s_in21ft1k',\n",
      " 'tf_efficientnetv2_s_in21k',\n",
      " 'tf_inception_v3',\n",
      " 'tf_mixnet_l',\n",
      " 'tf_mixnet_m',\n",
      " 'tf_mixnet_s',\n",
      " 'tf_mobilenetv3_large_075',\n",
      " 'tf_mobilenetv3_large_100',\n",
      " 'tf_mobilenetv3_large_minimal_100',\n",
      " 'tf_mobilenetv3_small_075',\n",
      " 'tf_mobilenetv3_small_100',\n",
      " 'tf_mobilenetv3_small_minimal_100',\n",
      " 'tnt_s_patch16_224',\n",
      " 'tresnet_l',\n",
      " 'tresnet_l_448',\n",
      " 'tresnet_m',\n",
      " 'tresnet_m_448',\n",
      " 'tresnet_m_miil_in21k',\n",
      " 'tresnet_xl',\n",
      " 'tresnet_xl_448',\n",
      " 'tv_densenet121',\n",
      " 'tv_resnet34',\n",
      " 'tv_resnet50',\n",
      " 'tv_resnet101',\n",
      " 'tv_resnet152',\n",
      " 'tv_resnext50_32x4d',\n",
      " 'twins_pcpvt_base',\n",
      " 'twins_pcpvt_large',\n",
      " 'twins_pcpvt_small',\n",
      " 'twins_svt_base',\n",
      " 'twins_svt_large',\n",
      " 'twins_svt_small',\n",
      " 'vgg11',\n",
      " 'vgg11_bn',\n",
      " 'vgg13',\n",
      " 'vgg13_bn',\n",
      " 'vgg16',\n",
      " 'vgg16_bn',\n",
      " 'vgg19',\n",
      " 'vgg19_bn',\n",
      " 'visformer_small',\n",
      " 'vit_base_patch16_224',\n",
      " 'vit_base_patch16_224_in21k',\n",
      " 'vit_base_patch16_224_miil',\n",
      " 'vit_base_patch16_224_miil_in21k',\n",
      " 'vit_base_patch16_384',\n",
      " 'vit_base_patch32_224',\n",
      " 'vit_base_patch32_224_in21k',\n",
      " 'vit_base_patch32_384',\n",
      " 'vit_base_r50_s16_224_in21k',\n",
      " 'vit_base_r50_s16_384',\n",
      " 'vit_huge_patch14_224_in21k',\n",
      " 'vit_large_patch16_224',\n",
      " 'vit_large_patch16_224_in21k',\n",
      " 'vit_large_patch16_384',\n",
      " 'vit_large_patch32_224_in21k',\n",
      " 'vit_large_patch32_384',\n",
      " 'vit_large_r50_s32_224',\n",
      " 'vit_large_r50_s32_224_in21k',\n",
      " 'vit_large_r50_s32_384',\n",
      " 'vit_small_patch16_224',\n",
      " 'vit_small_patch16_224_in21k',\n",
      " 'vit_small_patch16_384',\n",
      " 'vit_small_patch32_224',\n",
      " 'vit_small_patch32_224_in21k',\n",
      " 'vit_small_patch32_384',\n",
      " 'vit_small_r26_s32_224',\n",
      " 'vit_small_r26_s32_224_in21k',\n",
      " 'vit_small_r26_s32_384',\n",
      " 'vit_tiny_patch16_224',\n",
      " 'vit_tiny_patch16_224_in21k',\n",
      " 'vit_tiny_patch16_384',\n",
      " 'vit_tiny_r_s16_p8_224',\n",
      " 'vit_tiny_r_s16_p8_224_in21k',\n",
      " 'vit_tiny_r_s16_p8_384',\n",
      " 'wide_resnet50_2',\n",
      " 'wide_resnet101_2',\n",
      " 'xception',\n",
      " 'xception41',\n",
      " 'xception65',\n",
      " 'xception71']\n"
     ]
    }
   ],
   "source": [
    "model_names = timm.list_models(pretrained= True)\n",
    "from pprint import pprint\n",
    "pprint(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pre_logits): Identity()\n",
      "  (head): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=2)\n",
    "# model.eval()\n",
    "pprint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model = nn.Sequential(*list(model.children())[1:]) #丢弃PatchEmbed层，直接输入(B,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Dropout-1              [-1, 17, 768]               0\n",
      "         LayerNorm-2              [-1, 17, 768]           1,536\n",
      "            Linear-3             [-1, 17, 2304]       1,771,776\n",
      "           Dropout-4           [-1, 12, 17, 17]               0\n",
      "            Linear-5              [-1, 17, 768]         590,592\n",
      "           Dropout-6              [-1, 17, 768]               0\n",
      "         Attention-7              [-1, 17, 768]               0\n",
      "          Identity-8              [-1, 17, 768]               0\n",
      "         LayerNorm-9              [-1, 17, 768]           1,536\n",
      "           Linear-10             [-1, 17, 3072]       2,362,368\n",
      "             GELU-11             [-1, 17, 3072]               0\n",
      "          Dropout-12             [-1, 17, 3072]               0\n",
      "           Linear-13              [-1, 17, 768]       2,360,064\n",
      "          Dropout-14              [-1, 17, 768]               0\n",
      "              Mlp-15              [-1, 17, 768]               0\n",
      "         Identity-16              [-1, 17, 768]               0\n",
      "            Block-17              [-1, 17, 768]               0\n",
      "        LayerNorm-18              [-1, 17, 768]           1,536\n",
      "           Linear-19             [-1, 17, 2304]       1,771,776\n",
      "          Dropout-20           [-1, 12, 17, 17]               0\n",
      "           Linear-21              [-1, 17, 768]         590,592\n",
      "          Dropout-22              [-1, 17, 768]               0\n",
      "        Attention-23              [-1, 17, 768]               0\n",
      "         Identity-24              [-1, 17, 768]               0\n",
      "        LayerNorm-25              [-1, 17, 768]           1,536\n",
      "           Linear-26             [-1, 17, 3072]       2,362,368\n",
      "             GELU-27             [-1, 17, 3072]               0\n",
      "          Dropout-28             [-1, 17, 3072]               0\n",
      "           Linear-29              [-1, 17, 768]       2,360,064\n",
      "          Dropout-30              [-1, 17, 768]               0\n",
      "              Mlp-31              [-1, 17, 768]               0\n",
      "         Identity-32              [-1, 17, 768]               0\n",
      "            Block-33              [-1, 17, 768]               0\n",
      "        LayerNorm-34              [-1, 17, 768]           1,536\n",
      "           Linear-35             [-1, 17, 2304]       1,771,776\n",
      "          Dropout-36           [-1, 12, 17, 17]               0\n",
      "           Linear-37              [-1, 17, 768]         590,592\n",
      "          Dropout-38              [-1, 17, 768]               0\n",
      "        Attention-39              [-1, 17, 768]               0\n",
      "         Identity-40              [-1, 17, 768]               0\n",
      "        LayerNorm-41              [-1, 17, 768]           1,536\n",
      "           Linear-42             [-1, 17, 3072]       2,362,368\n",
      "             GELU-43             [-1, 17, 3072]               0\n",
      "          Dropout-44             [-1, 17, 3072]               0\n",
      "           Linear-45              [-1, 17, 768]       2,360,064\n",
      "          Dropout-46              [-1, 17, 768]               0\n",
      "              Mlp-47              [-1, 17, 768]               0\n",
      "         Identity-48              [-1, 17, 768]               0\n",
      "            Block-49              [-1, 17, 768]               0\n",
      "        LayerNorm-50              [-1, 17, 768]           1,536\n",
      "           Linear-51             [-1, 17, 2304]       1,771,776\n",
      "          Dropout-52           [-1, 12, 17, 17]               0\n",
      "           Linear-53              [-1, 17, 768]         590,592\n",
      "          Dropout-54              [-1, 17, 768]               0\n",
      "        Attention-55              [-1, 17, 768]               0\n",
      "         Identity-56              [-1, 17, 768]               0\n",
      "        LayerNorm-57              [-1, 17, 768]           1,536\n",
      "           Linear-58             [-1, 17, 3072]       2,362,368\n",
      "             GELU-59             [-1, 17, 3072]               0\n",
      "          Dropout-60             [-1, 17, 3072]               0\n",
      "           Linear-61              [-1, 17, 768]       2,360,064\n",
      "          Dropout-62              [-1, 17, 768]               0\n",
      "              Mlp-63              [-1, 17, 768]               0\n",
      "         Identity-64              [-1, 17, 768]               0\n",
      "            Block-65              [-1, 17, 768]               0\n",
      "        LayerNorm-66              [-1, 17, 768]           1,536\n",
      "           Linear-67             [-1, 17, 2304]       1,771,776\n",
      "          Dropout-68           [-1, 12, 17, 17]               0\n",
      "           Linear-69              [-1, 17, 768]         590,592\n",
      "          Dropout-70              [-1, 17, 768]               0\n",
      "        Attention-71              [-1, 17, 768]               0\n",
      "         Identity-72              [-1, 17, 768]               0\n",
      "        LayerNorm-73              [-1, 17, 768]           1,536\n",
      "           Linear-74             [-1, 17, 3072]       2,362,368\n",
      "             GELU-75             [-1, 17, 3072]               0\n",
      "          Dropout-76             [-1, 17, 3072]               0\n",
      "           Linear-77              [-1, 17, 768]       2,360,064\n",
      "          Dropout-78              [-1, 17, 768]               0\n",
      "              Mlp-79              [-1, 17, 768]               0\n",
      "         Identity-80              [-1, 17, 768]               0\n",
      "            Block-81              [-1, 17, 768]               0\n",
      "        LayerNorm-82              [-1, 17, 768]           1,536\n",
      "           Linear-83             [-1, 17, 2304]       1,771,776\n",
      "          Dropout-84           [-1, 12, 17, 17]               0\n",
      "           Linear-85              [-1, 17, 768]         590,592\n",
      "          Dropout-86              [-1, 17, 768]               0\n",
      "        Attention-87              [-1, 17, 768]               0\n",
      "         Identity-88              [-1, 17, 768]               0\n",
      "        LayerNorm-89              [-1, 17, 768]           1,536\n",
      "           Linear-90             [-1, 17, 3072]       2,362,368\n",
      "             GELU-91             [-1, 17, 3072]               0\n",
      "          Dropout-92             [-1, 17, 3072]               0\n",
      "           Linear-93              [-1, 17, 768]       2,360,064\n",
      "          Dropout-94              [-1, 17, 768]               0\n",
      "              Mlp-95              [-1, 17, 768]               0\n",
      "         Identity-96              [-1, 17, 768]               0\n",
      "            Block-97              [-1, 17, 768]               0\n",
      "        LayerNorm-98              [-1, 17, 768]           1,536\n",
      "           Linear-99             [-1, 17, 2304]       1,771,776\n",
      "         Dropout-100           [-1, 12, 17, 17]               0\n",
      "          Linear-101              [-1, 17, 768]         590,592\n",
      "         Dropout-102              [-1, 17, 768]               0\n",
      "       Attention-103              [-1, 17, 768]               0\n",
      "        Identity-104              [-1, 17, 768]               0\n",
      "       LayerNorm-105              [-1, 17, 768]           1,536\n",
      "          Linear-106             [-1, 17, 3072]       2,362,368\n",
      "            GELU-107             [-1, 17, 3072]               0\n",
      "         Dropout-108             [-1, 17, 3072]               0\n",
      "          Linear-109              [-1, 17, 768]       2,360,064\n",
      "         Dropout-110              [-1, 17, 768]               0\n",
      "             Mlp-111              [-1, 17, 768]               0\n",
      "        Identity-112              [-1, 17, 768]               0\n",
      "           Block-113              [-1, 17, 768]               0\n",
      "       LayerNorm-114              [-1, 17, 768]           1,536\n",
      "          Linear-115             [-1, 17, 2304]       1,771,776\n",
      "         Dropout-116           [-1, 12, 17, 17]               0\n",
      "          Linear-117              [-1, 17, 768]         590,592\n",
      "         Dropout-118              [-1, 17, 768]               0\n",
      "       Attention-119              [-1, 17, 768]               0\n",
      "        Identity-120              [-1, 17, 768]               0\n",
      "       LayerNorm-121              [-1, 17, 768]           1,536\n",
      "          Linear-122             [-1, 17, 3072]       2,362,368\n",
      "            GELU-123             [-1, 17, 3072]               0\n",
      "         Dropout-124             [-1, 17, 3072]               0\n",
      "          Linear-125              [-1, 17, 768]       2,360,064\n",
      "         Dropout-126              [-1, 17, 768]               0\n",
      "             Mlp-127              [-1, 17, 768]               0\n",
      "        Identity-128              [-1, 17, 768]               0\n",
      "           Block-129              [-1, 17, 768]               0\n",
      "       LayerNorm-130              [-1, 17, 768]           1,536\n",
      "          Linear-131             [-1, 17, 2304]       1,771,776\n",
      "         Dropout-132           [-1, 12, 17, 17]               0\n",
      "          Linear-133              [-1, 17, 768]         590,592\n",
      "         Dropout-134              [-1, 17, 768]               0\n",
      "       Attention-135              [-1, 17, 768]               0\n",
      "        Identity-136              [-1, 17, 768]               0\n",
      "       LayerNorm-137              [-1, 17, 768]           1,536\n",
      "          Linear-138             [-1, 17, 3072]       2,362,368\n",
      "            GELU-139             [-1, 17, 3072]               0\n",
      "         Dropout-140             [-1, 17, 3072]               0\n",
      "          Linear-141              [-1, 17, 768]       2,360,064\n",
      "         Dropout-142              [-1, 17, 768]               0\n",
      "             Mlp-143              [-1, 17, 768]               0\n",
      "        Identity-144              [-1, 17, 768]               0\n",
      "           Block-145              [-1, 17, 768]               0\n",
      "       LayerNorm-146              [-1, 17, 768]           1,536\n",
      "          Linear-147             [-1, 17, 2304]       1,771,776\n",
      "         Dropout-148           [-1, 12, 17, 17]               0\n",
      "          Linear-149              [-1, 17, 768]         590,592\n",
      "         Dropout-150              [-1, 17, 768]               0\n",
      "       Attention-151              [-1, 17, 768]               0\n",
      "        Identity-152              [-1, 17, 768]               0\n",
      "       LayerNorm-153              [-1, 17, 768]           1,536\n",
      "          Linear-154             [-1, 17, 3072]       2,362,368\n",
      "            GELU-155             [-1, 17, 3072]               0\n",
      "         Dropout-156             [-1, 17, 3072]               0\n",
      "          Linear-157              [-1, 17, 768]       2,360,064\n",
      "         Dropout-158              [-1, 17, 768]               0\n",
      "             Mlp-159              [-1, 17, 768]               0\n",
      "        Identity-160              [-1, 17, 768]               0\n",
      "           Block-161              [-1, 17, 768]               0\n",
      "       LayerNorm-162              [-1, 17, 768]           1,536\n",
      "          Linear-163             [-1, 17, 2304]       1,771,776\n",
      "         Dropout-164           [-1, 12, 17, 17]               0\n",
      "          Linear-165              [-1, 17, 768]         590,592\n",
      "         Dropout-166              [-1, 17, 768]               0\n",
      "       Attention-167              [-1, 17, 768]               0\n",
      "        Identity-168              [-1, 17, 768]               0\n",
      "       LayerNorm-169              [-1, 17, 768]           1,536\n",
      "          Linear-170             [-1, 17, 3072]       2,362,368\n",
      "            GELU-171             [-1, 17, 3072]               0\n",
      "         Dropout-172             [-1, 17, 3072]               0\n",
      "          Linear-173              [-1, 17, 768]       2,360,064\n",
      "         Dropout-174              [-1, 17, 768]               0\n",
      "             Mlp-175              [-1, 17, 768]               0\n",
      "        Identity-176              [-1, 17, 768]               0\n",
      "           Block-177              [-1, 17, 768]               0\n",
      "       LayerNorm-178              [-1, 17, 768]           1,536\n",
      "          Linear-179             [-1, 17, 2304]       1,771,776\n",
      "         Dropout-180           [-1, 12, 17, 17]               0\n",
      "          Linear-181              [-1, 17, 768]         590,592\n",
      "         Dropout-182              [-1, 17, 768]               0\n",
      "       Attention-183              [-1, 17, 768]               0\n",
      "        Identity-184              [-1, 17, 768]               0\n",
      "       LayerNorm-185              [-1, 17, 768]           1,536\n",
      "          Linear-186             [-1, 17, 3072]       2,362,368\n",
      "            GELU-187             [-1, 17, 3072]               0\n",
      "         Dropout-188             [-1, 17, 3072]               0\n",
      "          Linear-189              [-1, 17, 768]       2,360,064\n",
      "         Dropout-190              [-1, 17, 768]               0\n",
      "             Mlp-191              [-1, 17, 768]               0\n",
      "        Identity-192              [-1, 17, 768]               0\n",
      "           Block-193              [-1, 17, 768]               0\n",
      "       LayerNorm-194              [-1, 17, 768]           1,536\n",
      "        Identity-195              [-1, 17, 768]               0\n",
      "          Linear-196                [-1, 17, 2]           1,538\n",
      "================================================================\n",
      "Total params: 85,057,538\n",
      "Trainable params: 85,057,538\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 31.69\n",
      "Params size (MB): 324.47\n",
      "Estimated Total Size (MB): 356.21\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vit_model=vit_model.to(device)\n",
    "summary(vit_model, (17,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch._C.Node' object has no attribute 'ival'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13568/2055002243.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorwatch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/my_torch/lib/python3.8/site-packages/tensorwatch/__init__.py\u001b[0m in \u001b[0;36mdraw_model\u001b[0;34m(model, input_shape, orientation, png_filename)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdraw_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'TB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpng_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#orientation = 'LR' for landscpe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhiddenlayer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_draw_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytorch_draw_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_torch/lib/python3.8/site-packages/tensorwatch/model_graph/hiddenlayer/pytorch_draw_model.py\u001b[0m in \u001b[0;36mdraw_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_img_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mDotWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_torch/lib/python3.8/site-packages/tensorwatch/model_graph/hiddenlayer/pytorch_draw_model.py\u001b[0m in \u001b[0;36mdraw_img_classifier\u001b[0;34m(model, dataset, display_param_nodes, rankdir, styles, input_shape)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mnon_para_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistiller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_non_parallel_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_para_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msgraph2dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_param_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_torch/lib/python3.8/site-packages/tensorwatch/model_graph/hiddenlayer/summary_graph.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, dummy_input, apply_scope_name_workarounds)\u001b[0m\n\u001b[1;32m    219\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSummaryGraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEdge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_op\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0mnew_op\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attrs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributeNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__merge_pad_avgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_torch/lib/python3.8/site-packages/tensorwatch/model_graph/hiddenlayer/summary_graph.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    219\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSummaryGraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEdge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_op\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0mnew_op\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attrs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributeNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__merge_pad_avgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_torch/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_node_getitem\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m   1143\u001b[0m     \"\"\"\n\u001b[1;32m   1144\u001b[0m     \u001b[0msel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkindOf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch._C.Node' object has no attribute 'ival'"
     ]
    }
   ],
   "source": [
    "import tensorwatch as tw\n",
    "tw.draw_model(model, [1, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sequential(\n",
      "  (0): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (2): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (3): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (4): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (5): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (6): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (7): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (8): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (9): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (10): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (11): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "),\n",
      " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
      " Identity(),\n",
      " Linear(in_features=768, out_features=2, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(model.children())[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model = nn.Sequential(*list(model.children()))  #丢弃PatchEmbed层，直接输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "0.proj.weight \t torch.Size([768, 3, 16, 16])\n",
      "0.proj.bias \t torch.Size([768])\n",
      "2.0.norm1.weight \t torch.Size([768])\n",
      "2.0.norm1.bias \t torch.Size([768])\n",
      "2.0.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "2.0.attn.qkv.bias \t torch.Size([2304])\n",
      "2.0.attn.proj.weight \t torch.Size([768, 768])\n",
      "2.0.attn.proj.bias \t torch.Size([768])\n",
      "2.0.norm2.weight \t torch.Size([768])\n",
      "2.0.norm2.bias \t torch.Size([768])\n",
      "2.0.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "2.0.mlp.fc1.bias \t torch.Size([3072])\n",
      "2.0.mlp.fc2.weight \t torch.Size([768, 3072])\n",
      "2.0.mlp.fc2.bias \t torch.Size([768])\n",
      "2.1.norm1.weight \t torch.Size([768])\n",
      "2.1.norm1.bias \t torch.Size([768])\n",
      "2.1.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "2.1.attn.qkv.bias \t torch.Size([2304])\n",
      "2.1.attn.proj.weight \t torch.Size([768, 768])\n",
      "2.1.attn.proj.bias \t torch.Size([768])\n",
      "2.1.norm2.weight \t torch.Size([768])\n",
      "2.1.norm2.bias \t torch.Size([768])\n",
      "2.1.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "2.1.mlp.fc1.bias \t torch.Size([3072])\n",
      "2.1.mlp.fc2.weight \t torch.Size([768, 3072])\n",
      "2.1.mlp.fc2.bias \t torch.Size([768])\n",
      "2.2.norm1.weight \t torch.Size([768])\n",
      "2.2.norm1.bias \t torch.Size([768])\n",
      "2.2.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "2.2.attn.qkv.bias \t torch.Size([2304])\n",
      "2.2.attn.proj.weight \t torch.Size([768, 768])\n",
      "2.2.attn.proj.bias \t torch.Size([768])\n",
      "2.2.norm2.weight \t torch.Size([768])\n",
      "2.2.norm2.bias \t torch.Size([768])\n",
      "2.2.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "2.2.mlp.fc1.bias \t torch.Size([3072])\n",
      "2.2.mlp.fc2.weight \t torch.Size([768, 3072])\n",
      "2.2.mlp.fc2.bias \t torch.Size([768])\n",
      "2.3.norm1.weight \t torch.Size([768])\n",
      "2.3.norm1.bias \t torch.Size([768])\n",
      "2.3.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "2.3.attn.qkv.bias \t torch.Size([2304])\n",
      "2.3.attn.proj.weight \t torch.Size([768, 768])\n",
      "2.3.attn.proj.bias \t torch.Size([768])\n",
      "2.3.norm2.weight \t torch.Size([768])\n",
      "2.3.norm2.bias \t torch.Size([768])\n",
      "2.3.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "2.3.mlp.fc1.bias \t torch.Size([3072])\n",
      "2.3.mlp.fc2.weight \t torch.Size([768, 3072])\n",
      "2.3.mlp.fc2.bias \t torch.Size([768])\n",
      "2.4.norm1.weight \t torch.Size([768])\n",
      "2.4.norm1.bias \t torch.Size([768])\n",
      "2.4.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "2.4.attn.qkv.bias \t torch.Size([2304])\n",
      "2.4.attn.proj.weight \t torch.Size([768, 768])\n",
      "2.4.attn.proj.bias \t torch.Size([768])\n",
      "2.4.norm2.weight \t torch.Size([768])\n",
      "2.4.norm2.bias \t torch.Size([768])\n",
      "2.4.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "2.4.mlp.fc1.bias \t torch.Size([3072])\n",
      "2.4.mlp.fc2.weight \t torch.Size([768, 3072])\n",
      "2.4.mlp.fc2.bias \t torch.Size([768])\n",
      "2.5.norm1.weight \t torch.Size([768])\n",
      "2.5.norm1.bias \t torch.Size([768])\n",
      "2.5.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "2.5.attn.qkv.bias \t torch.Size([2304])\n",
      "2.5.attn.proj.weight \t torch.Size([768, 768])\n",
      "2.5.attn.proj.bias \t torch.Size([768])\n",
      "2.5.norm2.weight \t torch.Size([768])\n",
      "2.5.norm2.bias \t torch.Size([768])\n",
      "2.5.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "2.5.mlp.fc1.bias \t torch.Size([3072])\n",
      "2.5.mlp.fc2.weight \t torch.Size([768, 3072])\n",
      "2.5.mlp.fc2.bias \t torch.Size([768])\n",
      "2.6.norm1.weight \t torch.Size([768])\n",
      "2.6.norm1.bias \t torch.Size([768])\n",
      "2.6.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "2.6.attn.qkv.bias \t torch.Size([2304])\n",
      "2.6.attn.proj.weight \t torch.Size([768, 768])\n",
      "2.6.attn.proj.bias \t torch.Size([768])\n",
      "2.6.norm2.weight \t torch.Size([768])\n",
      "2.6.norm2.bias \t torch.Size([768])\n",
      "2.6.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "2.6.mlp.fc1.bias \t torch.Size([3072])\n",
      "2.6.mlp.fc2.weight \t torch.Size([768, 3072])\n",
      "2.6.mlp.fc2.bias \t torch.Size([768])\n",
      "2.7.norm1.weight \t torch.Size([768])\n",
      "2.7.norm1.bias \t torch.Size([768])\n",
      "2.7.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "2.7.attn.qkv.bias \t torch.Size([2304])\n",
      "2.7.attn.proj.weight \t torch.Size([768, 768])\n",
      "2.7.attn.proj.bias \t torch.Size([768])\n",
      "2.7.norm2.weight \t torch.Size([768])\n",
      "2.7.norm2.bias \t torch.Size([768])\n",
      "2.7.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "2.7.mlp.fc1.bias \t torch.Size([3072])\n",
      "2.7.mlp.fc2.weight \t torch.Size([768, 3072])\n",
      "2.7.mlp.fc2.bias \t torch.Size([768])\n",
      "2.8.norm1.weight \t torch.Size([768])\n",
      "2.8.norm1.bias \t torch.Size([768])\n",
      "2.8.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "2.8.attn.qkv.bias \t torch.Size([2304])\n",
      "2.8.attn.proj.weight \t torch.Size([768, 768])\n",
      "2.8.attn.proj.bias \t torch.Size([768])\n",
      "2.8.norm2.weight \t torch.Size([768])\n",
      "2.8.norm2.bias \t torch.Size([768])\n",
      "2.8.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "2.8.mlp.fc1.bias \t torch.Size([3072])\n",
      "2.8.mlp.fc2.weight \t torch.Size([768, 3072])\n",
      "2.8.mlp.fc2.bias \t torch.Size([768])\n",
      "2.9.norm1.weight \t torch.Size([768])\n",
      "2.9.norm1.bias \t torch.Size([768])\n",
      "2.9.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "2.9.attn.qkv.bias \t torch.Size([2304])\n",
      "2.9.attn.proj.weight \t torch.Size([768, 768])\n",
      "2.9.attn.proj.bias \t torch.Size([768])\n",
      "2.9.norm2.weight \t torch.Size([768])\n",
      "2.9.norm2.bias \t torch.Size([768])\n",
      "2.9.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "2.9.mlp.fc1.bias \t torch.Size([3072])\n",
      "2.9.mlp.fc2.weight \t torch.Size([768, 3072])\n",
      "2.9.mlp.fc2.bias \t torch.Size([768])\n",
      "2.10.norm1.weight \t torch.Size([768])\n",
      "2.10.norm1.bias \t torch.Size([768])\n",
      "2.10.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "2.10.attn.qkv.bias \t torch.Size([2304])\n",
      "2.10.attn.proj.weight \t torch.Size([768, 768])\n",
      "2.10.attn.proj.bias \t torch.Size([768])\n",
      "2.10.norm2.weight \t torch.Size([768])\n",
      "2.10.norm2.bias \t torch.Size([768])\n",
      "2.10.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "2.10.mlp.fc1.bias \t torch.Size([3072])\n",
      "2.10.mlp.fc2.weight \t torch.Size([768, 3072])\n",
      "2.10.mlp.fc2.bias \t torch.Size([768])\n",
      "2.11.norm1.weight \t torch.Size([768])\n",
      "2.11.norm1.bias \t torch.Size([768])\n",
      "2.11.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "2.11.attn.qkv.bias \t torch.Size([2304])\n",
      "2.11.attn.proj.weight \t torch.Size([768, 768])\n",
      "2.11.attn.proj.bias \t torch.Size([768])\n",
      "2.11.norm2.weight \t torch.Size([768])\n",
      "2.11.norm2.bias \t torch.Size([768])\n",
      "2.11.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "2.11.mlp.fc1.bias \t torch.Size([3072])\n",
      "2.11.mlp.fc2.weight \t torch.Size([768, 3072])\n",
      "2.11.mlp.fc2.bias \t torch.Size([768])\n",
      "3.weight \t torch.Size([768])\n",
      "3.bias \t torch.Size([768])\n",
      "5.weight \t torch.Size([2, 768])\n",
      "5.bias \t torch.Size([2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in vit_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", vit_model.state_dict()[param_tensor].size())\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diode/anaconda3/envs/my_torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from PIL import Image\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "config = resolve_data_config({}, model=model)\n",
    "transform = create_transform(**config)\n",
    "\n",
    "url, filename = (\"\", \"dog.jpg\")\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "img = Image.open(filename).convert('RGB')\n",
    "tensor = transform(img).unsqueeze(0) # transform and add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    out = model(tensor)\n",
    "probabilities = torch.nn.functional.softmax(out[0], dim=0)\n",
    "print(probabilities.shape)\n",
    "# prints: torch.Size([1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samoyed 0.5055654644966125\n",
      "Pomeranian 0.31216004490852356\n",
      "keeshond 0.060011763125658035\n",
      "Pekinese 0.01597055420279503\n",
      "white wolf 0.012953994795680046\n"
     ]
    }
   ],
   "source": [
    "# Get imagenet class mappings\n",
    "url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\n",
    "urllib.request.urlretrieve(url, filename) \n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "\n",
    "# Print top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())\n",
    "# prints class names and probabilities like:\n",
    "# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(120.).reshape(3,2,4,5)\n",
    "b = np.arange(120.).reshape(3,2,4,5)\n",
    " \n",
    "# 语义解析：\n",
    "# 输入a：4阶张量，下标为bhid\n",
    "# 输入b: 4阶张量，下标为bhjd\n",
    "# 输出o: 4阶张量，下标为bhij\n",
    "# 隐含语义：对d进行求和，公式附于代码之后：\n",
    "o = np.einsum('bhid,bhjd->bhij', a, b)\n",
    "print(o.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.0\n",
      "255.0\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(a[0,0,1,:]*b[0,0,1,:].T))\n",
    "print(o[0,0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6078, 0.3124, 0.5947],\n",
      "        [0.3450, 0.1349, 0.8827],\n",
      "        [0.9812, 0.3748, 0.6346]])\n",
      "torch.Size([3, 3])\n",
      "(tensor([0.6078, 0.3124, 0.5947]), tensor([0.3450, 0.1349, 0.8827]), tensor([0.9812, 0.3748, 0.6346]))\n",
      "(tensor([0.6078, 0.3450, 0.9812]), tensor([0.3124, 0.1349, 0.3748]), tensor([0.5947, 0.8827, 0.6346]))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "t = torch.rand(3,3) #随机生成一个tensor\n",
    "print(t)\n",
    "print(t.shape)\n",
    "r = torch.unbind(t,dim=0)#dim = 0指定拆除的维度\n",
    "print(r)\n",
    "s = torch.unbind(t,dim=1)#dim = 1指定拆除的维度\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9378, 0.4521, 0.8406, 0.7869, 0.3964],\n",
      "         [0.1137, 0.9675, 0.5701, 0.2221, 0.0729],\n",
      "         [0.6036, 0.0851, 0.1466, 0.3652, 0.5866]],\n",
      "\n",
      "        [[0.8572, 0.0210, 0.9551, 0.8797, 0.0433],\n",
      "         [0.0696, 0.1387, 0.0579, 0.6885, 0.8274],\n",
      "         [0.3953, 0.5798, 0.8748, 0.2505, 0.0199]],\n",
      "\n",
      "        [[0.3884, 0.5439, 0.6301, 0.1447, 0.7654],\n",
      "         [0.5371, 0.2055, 0.9017, 0.8197, 0.8602],\n",
      "         [0.3711, 0.8924, 0.1424, 0.5873, 0.9300]],\n",
      "\n",
      "        [[0.0019, 0.5562, 0.5870, 0.0367, 0.8611],\n",
      "         [0.5079, 0.0713, 0.2364, 0.4917, 0.6602],\n",
      "         [0.5049, 0.1970, 0.3299, 0.8698, 0.4369]],\n",
      "\n",
      "        [[0.8740, 0.0685, 0.0114, 0.8118, 0.8106],\n",
      "         [0.2367, 0.4326, 0.4672, 0.5138, 0.3092],\n",
      "         [0.1055, 0.8105, 0.4843, 0.2616, 0.1116]]])\n",
      "tensor([[[0.9378, 0.4521, 0.8406, 0.7869, 0.3964],\n",
      "         [0.1137, 0.9675, 0.5701, 0.2221, 0.0729],\n",
      "         [0.6036, 0.0851, 0.1466, 0.3652, 0.5866]],\n",
      "\n",
      "        [[0.8572, 0.0210, 0.9551, 0.8797, 0.0433],\n",
      "         [0.0696, 0.1387, 0.0579, 0.6885, 0.8274],\n",
      "         [0.3953, 0.5798, 0.8748, 0.2505, 0.0199]],\n",
      "\n",
      "        [[0.3884, 0.5439, 0.6301, 0.1447, 0.7654],\n",
      "         [0.5371, 0.2055, 0.9017, 0.8197, 0.8602],\n",
      "         [0.3711, 0.8924, 0.1424, 0.5873, 0.9300]],\n",
      "\n",
      "        [[0.0019, 0.5562, 0.5870, 0.0367, 0.8611],\n",
      "         [0.5079, 0.0713, 0.2364, 0.4917, 0.6602],\n",
      "         [0.5049, 0.1970, 0.3299, 0.8698, 0.4369]],\n",
      "\n",
      "        [[0.8740, 0.0685, 0.0114, 0.8118, 0.8106],\n",
      "         [0.2367, 0.4326, 0.4672, 0.5138, 0.3092],\n",
      "         [0.1055, 0.8105, 0.4843, 0.2616, 0.1116]]])\n"
     ]
    }
   ],
   "source": [
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "t = torch.rand(5,3,5) #随机生成一个tensor\n",
    "print(t)\n",
    "dt = drop_path(t, 11, True)\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(math.floor(28/6))\n",
    "print(1000//1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a=torch.tensor([[1,2,3],[4,5,6]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1, 2, 3],\n",
      "        [4, 5, 6]]), tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])]\n"
     ]
    }
   ],
   "source": [
    "b = []\n",
    "b.append(a)\n",
    "b.append(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29877/3961611505.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "c=np.array(b)\n",
    "d=torch.from_numpy(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "d = torch.stack(b, dim=0)\n",
    "print(d.shape[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82d58a670d6c1da2346d10f1c1f705e83a4d54ecb0aff91385168b8ee904ab6a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('my_torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
